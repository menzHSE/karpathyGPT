{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See:\n",
    "https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check the devices that we have available and prefer CUDA over MPS and CPU\n",
    "def autoselectDevice(verbose=1):\n",
    "\n",
    "    # default: CPU\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # CUDA\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        # MPS (acceleration on Apple silicon M1 / M2 chips)\n",
    "        device = torch.device('mps')\n",
    "\n",
    "    if verbose:\n",
    "        print('Using device:', device)\n",
    "\n",
    "    # Additional Info when using cuda\n",
    "    if verbose and device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "\n",
    "    return device\n",
    "\n",
    "# We transfer our model and data later to this device. If this is a GPU\n",
    "# PyTorch will take care of everything automatically.\n",
    "device = autoselectDevice(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size is 65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(f\"Vocab size is {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "# encoder and decoder\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktoken encoding\n",
    "#import tiktoken\n",
    "#ttenc = tiktoken.get_encoding(\"gpt-4\")\n",
    "#encode = lambda s: ttenc.encode(s)\n",
    "#decode = lambda l: ttenc.decode(l)\n",
    "#vocab_size = ttenc.n_vocab\n",
    "#print(f\"Vocab size is {vocab_size}\")\n",
    "\n",
    "#print(encode(\"hii there\"))\n",
    "#print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is 1024\n",
      "[191, 53, 819]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "import tokenmonster\n",
    "\n",
    "# Optionally set the tokenmonster directory, otherwise it will use ~/_tokenmonster\n",
    "tokenmonster.set_local_directory(\"./_tokenmonster\")\n",
    "\n",
    "# Load a vocabulary by name, filepath or URL\n",
    "ttenc = tokenmonster.load(\"english-1024-consistent-v1\")\n",
    "\n",
    "\n",
    "encode = lambda s: list(ttenc.tokenize(s))\n",
    "decode = lambda l: ttenc.decode(l)\n",
    "vocab_size = ttenc.vocab_size\n",
    "print(f\"Vocab size is {vocab_size}\")\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([485471]) torch.int64\n",
      "tensor([ 156,  786,   36,    3,  171,  243,  255,   58,   29,  260,  853,  384,\n",
      "         358,   37,  349,   49,  178,  466,  311,  233,  415,   15,  669,  339,\n",
      "         568,   45,   55,  761,  464,   29,  260,  568,   45,   55,   15,  568,\n",
      "          45,   55,  761,  786,   36,    3,  171,  243,  255,   58,   29,  260,\n",
      "         956,  464,  361,   37,  565,  251,   48,  360,  622,  377,  494,  734,\n",
      "         377,  306,   37,  340,   37,  367,   34,  451,  464,   29,  260,  361,\n",
      "          37,  565,  251,   48,   17,  361,   37,  565,  251,   48,  761,  786,\n",
      "          36,    3,  171,  243,  255,   58,   29,  260,  786,   15,  592,  681,\n",
      "          36,  288,   53,  249,   36,  523,  171,  249,  326,  482,   49,   50,\n",
      "         302,   49,   37,  343,  903,  887,  761,  464,   29,  260,  384,  681,\n",
      "          10,   64,   15,  384,  681,   10,   64,  761,  786,   36,    3,  171,\n",
      "         243,  255,   58,   29,  260,  520,  380,  333,  206,  512,   15,  465,\n",
      "         384,   10,  206,  667,  486,   58,  279,  540,  542,  548,  169,  595,\n",
      "         326,   10,   64,  117,  586,  176,  173,   34,  451,  464,   29,  260,\n",
      "         346,  698,  728,  418,  351,   10,   64,   30,  520,  327,  282,  496,\n",
      "          49,   29,  632,   15,  632,    4,  451,  896,   36,    3,  171,  243,\n",
      "         255,   58,   29,  260,  539,  757,   15,  665,    3,  171,  243,  255,\n",
      "         218,  761,  786,   36,    3,  171,  243,  255,   58,   29,  260,  384,\n",
      "         468,  270,   37,  782,  178,  357,  222,    3,  171,  243,  255,  218,\n",
      "          15,  577,  354,   37,  580,  171,  164,   63,  665,  595,  753,  280,\n",
      "         242,  222,  201,   69,  576,  185,   37,  517,  351,  839,  361,  205,\n",
      "          49,  251,  380,   29,  323,  738,  103,  839,  140,  196,   56,   48,\n",
      "         380,  476,  577,  134,   37,  379,  182,   50,   56,   65,  201,  253,\n",
      "         837,  327,  752,  103,  590,  204,   37,  722,   15,  384,  798,  317,\n",
      "          49,  238,  738,  361,  205,   49,  251,   48,  380,  322,  426,  179,\n",
      "          69,   30,  103,  476,  738,  822,  384,  468,  578,  120,   37,  497,\n",
      "          29,  577,  519,   58,  611,  735,  103,  117,   50,   50,  205,  173,\n",
      "          63,  380,   15,  577,  882,  350,  540,  530,  182,  253,  326,  278,\n",
      "         275,  103,  325,  251,  219,  222,   69,  377, 1010,  200,   49,  818,\n",
      "         117,   37,  118,  443,  600,   30,  540,  103,  372,   50,  185,   37,\n",
      "         552,  169,  678,  312,  199,  377,  736,   36,  520,  380,  133,   37,\n",
      "         653,  188,  739,  756,  103,  540,  356,  202,  235,  303,   49,  384,\n",
      "         282,   37,  646,  360,  202,   63,   29,  921,  315,   48,   63,  681,\n",
      "         397,  103,  568,   45,   55,  739,  325,  322,  215,  182,  505,  118,\n",
      "          37,  716,   15,  535,  325,  375,   53,  232,   64,  505,  133,   37,\n",
      "         653,  188,  761,  896,   36,    3,  171,  243,  255,   58,   29,  260,\n",
      "         839,  592,  358,   37,  349,   49,  178,  121,   37,  941,  208,  910,\n",
      "          36,  288,   53,  249,   36,  523,  171,  249,   34,  451,  464,   29,\n",
      "         260,  910,  512,  786,   29,  319,  142,  117,  746,  297,   51,  903,\n",
      "         484,   37,  531,  162,  246,  761,  896,   36,    3,  171,  243,  255,\n",
      "          58,   29,  260,  965,  592,  753,  939,   63,  319,  510,  496,   49,\n",
      "         505,  513,  782,  234,   34,  451,  786,   36,    3,  171,  243,  255,\n",
      "          58,   29,  260,  746,  750,   30,  465,  781,  282,  485,  440,   64,\n",
      "         377,  664,  512,  665,  103,  361,  613,  505,   64,   15,  476,  944,\n",
      "         354,   69,   63,  512,  618,  756,  778,  549,   65,   48,  761,  896,\n",
      "          36,    3,  171,  243,  255,   58,   29,  260,  344,  253,  476,  568,\n",
      "          45,   55,  535,  338,  205,  171,  432,  208,  761,  786,   36,    3,\n",
      "         171,  243,  255,   58,   29,    1,  597,  559,  378,  244,  592,   15,\n",
      "         753,  319,  318,  242,  496,   49,  306,   37,  341,  249,  208,   15,\n",
      "         319,  493,  103,  327,  377,  735,  500,   29,  901,  134,   37,  350,\n",
      "          64,  147,  485,   37,  365,  196,  429,   48,  526,  858,  103,  485,\n",
      "         440,   64,  377,  559,  876,  505,  513,  782,  234,  319,  493,  327,\n",
      "         377,  103,  132,   37,  519,  236,  513,  129,   37,  803,  465,  826,\n",
      "         710,  208,  549,   65,   48,   30,  836,  319,  103,  326,   15,  653,\n",
      "         376,  206,  577,  273,  243,   64,   65,  175,  883,  381,  233,   65,\n",
      "          49,  761,  896,   36,    3,  171,  243,  255,   58,   29,  260,  753,\n",
      "         319,  477,   37,  535,  670,  325,  513,  344,  625,   15,  592,  270,\n",
      "          37,  782,  117,  103,  381,  169,  325,  512,  150,  592,  342,  239,\n",
      "         325,  346,  589,  559,  319,  326,  291,  251,  244,  249,  761,  786,\n",
      "          36,    3,  171,  243,  255,   58,   29,  260,  323,  397,  342,  239,\n",
      "         535,   15,  397,  703,  535,  282,  472,  229,   58,  350,  270,   37,\n",
      "         293,   37,  364,  771,   30,  103,  319,  318,  242,  122,   37,  280,\n",
      "          56,  245,   15,  756,  576,  225,  249,   15,  377,  376,  229,  325,\n",
      "         361,  224,  243,  624,  595,  753,  367,   37,  541,  768,  820,   34,\n",
      "         763,  803,  720,  131,   10,  577,  645,  103,  326,  362,   37,  561,\n",
      "          29,  138,   52,   69,  569,   69,  384,  358,  166,  418,  671,   34,\n",
      "         903,   36,  288,   37,  356,  244,   56,    4,  451,  464,   29,  260,\n",
      "         646,   15,  646,  761,  786,   36,    3,  171,  243,  255,   58,   29,\n",
      "         260,  134,   37,  350,   64,    4,  590,  646,   63,  671,   34,  451,\n",
      "         896,   36,    3,  171,  243,  255,   58,   29,  260,  386,  233,   52,\n",
      "          69,   36,  526,  181,   53,  249,   36,  117,   37,  316,   53,   60,\n",
      "          37,  354,   30,  539,  735,  318,  242,  847,  691,   48,  103,  577,\n",
      "         887,  761,  786,   36,    3,  171,  243,  255,   58,   29,  260,  319,\n",
      "         142,  539,  321,   58,   37,  502,  302,   59,   65,   51,   52,   29,\n",
      "         839,  911,  557,   64,  752,  369,    4,  102,  157,  526,  181,   53,\n",
      "         249,   29,  260,  753,  758,  142,   15,  343,  782,  234,   37,  526,\n",
      "          15,  325,  666,   34,  835,  315,  592,  260,  756,  281,  245,  465,\n",
      "         290,   65,   46,   63,   34,  763,  524,  441,   34,  568,   45,   55,\n",
      "          15,  397,  358,   45,   69,  592,  761,  786,   36,    3,  171,  243,\n",
      "         255,   58,   29,  260,  540,  963,  326,  535,  378,   37,  681,   58,\n",
      "         903,  561,  401,   30,  738,  667,  103,  509,  325,   55,  205,  215,\n",
      "         739,  505,   64,   37,  801,  753,  384,  516,   37,  500,  827,   15,\n",
      "         103,  836,  536,  384,   10,  206,  719,    3,   10,  180,  325,  295,\n",
      "         178,  438,  738,  559])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([156]) the target: 786\n",
      "when input is tensor([156, 786]) the target: 36\n",
      "when input is tensor([156, 786,  36]) the target: 3\n",
      "when input is tensor([156, 786,  36,   3]) the target: 171\n",
      "when input is tensor([156, 786,  36,   3, 171]) the target: 243\n",
      "when input is tensor([156, 786,  36,   3, 171, 243]) the target: 255\n",
      "when input is tensor([156, 786,  36,   3, 171, 243, 255]) the target: 58\n",
      "when input is tensor([156, 786,  36,   3, 171, 243, 255,  58]) the target: 29\n"
     ]
    }
   ],
   "source": [
    "# this is \"context length\" or \"block size\" in GPT terminology\n",
    "block_size = 8\n",
    "# this has multiple (8) training samples inside of it\n",
    "train_data[:block_size+1]\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[541,  52,  15, 260, 318, 422, 541, 377],\n",
      "        [987, 359,  37, 563,  48, 541, 760, 312],\n",
      "        [920, 769, 513, 122,  37, 464, 605, 397],\n",
      "        [638, 354, 219, 204,  62,  15, 476, 204]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 52,  15, 260, 318, 422, 541, 377, 342],\n",
      "        [359,  37, 563,  48, 541, 760, 312, 241],\n",
      "        [769, 513, 122,  37, 464, 605, 397, 897],\n",
      "        [354, 219, 204,  62,  15, 476, 204,  62]])\n",
      "----\n",
      "when input is [541] the target: 52\n",
      "when input is [541, 52] the target: 15\n",
      "when input is [541, 52, 15] the target: 260\n",
      "when input is [541, 52, 15, 260] the target: 318\n",
      "when input is [541, 52, 15, 260, 318] the target: 422\n",
      "when input is [541, 52, 15, 260, 318, 422] the target: 541\n",
      "when input is [541, 52, 15, 260, 318, 422, 541] the target: 377\n",
      "when input is [541, 52, 15, 260, 318, 422, 541, 377] the target: 342\n",
      "when input is [987] the target: 359\n",
      "when input is [987, 359] the target: 37\n",
      "when input is [987, 359, 37] the target: 563\n",
      "when input is [987, 359, 37, 563] the target: 48\n",
      "when input is [987, 359, 37, 563, 48] the target: 541\n",
      "when input is [987, 359, 37, 563, 48, 541] the target: 760\n",
      "when input is [987, 359, 37, 563, 48, 541, 760] the target: 312\n",
      "when input is [987, 359, 37, 563, 48, 541, 760, 312] the target: 241\n",
      "when input is [920] the target: 769\n",
      "when input is [920, 769] the target: 513\n",
      "when input is [920, 769, 513] the target: 122\n",
      "when input is [920, 769, 513, 122] the target: 37\n",
      "when input is [920, 769, 513, 122, 37] the target: 464\n",
      "when input is [920, 769, 513, 122, 37, 464] the target: 605\n",
      "when input is [920, 769, 513, 122, 37, 464, 605] the target: 397\n",
      "when input is [920, 769, 513, 122, 37, 464, 605, 397] the target: 897\n",
      "when input is [638] the target: 354\n",
      "when input is [638, 354] the target: 219\n",
      "when input is [638, 354, 219] the target: 204\n",
      "when input is [638, 354, 219, 204] the target: 62\n",
      "when input is [638, 354, 219, 204, 62] the target: 15\n",
      "when input is [638, 354, 219, 204, 62, 15] the target: 476\n",
      "when input is [638, 354, 219, 204, 62, 15, 476] the target: 204\n",
      "when input is [638, 354, 219, 204, 62, 15, 476, 204] the target: 62\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[541,  52,  15, 260, 318, 422, 541, 377],\n",
      "        [987, 359,  37, 563,  48, 541, 760, 312],\n",
      "        [920, 769, 513, 122,  37, 464, 605, 397],\n",
      "        [638, 354, 219, 204,  62,  15, 476, 204]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 52,  15, 260, 318, 422, 541, 377, 342],\n",
      "        [359,  37, 563,  48, 541, 760, 312, 241],\n",
      "        [769, 513, 122,  37, 464, 605, 397, 897],\n",
      "        [354, 219, 204,  62,  15, 476, 204,  62]])\n"
     ]
    }
   ],
   "source": [
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024])\n",
      "tensor(7.3747, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# every index in xb will be embedded into a vector of length vocab_size\n",
    "# Interpretation: Every character is embedded into a vector of length vocab_size that\n",
    "# represents the probability distribution over the next character. We predict the next\n",
    "# character by taking the argmax of this distribution, i.e. solely based what the current\n",
    "# charcter is. This is a very simple model that will not be able to learn long-range\n",
    "# dependencies.\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)   \n",
    "\n",
    "        # cross-entropy loss. We want to predict the next token in the sequence. \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "            # channels are expected in the second dimension, thats why we need to flatten\n",
    "            # the (B,T,C) tensor into a (B*T,C) tensor\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)    \n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    # generate a sequence of tokens of length max_new_tokens, starting from idx\n",
    "    # idx is a (B,T) tensor of integers\n",
    "\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step because these are the predictions \n",
    "            # for the next token\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "  \n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t good do polie let solding\n",
      "               ction cor law\" any greatunque and that but Parathoughtor string em of the eas was unp att pes to word is a event $\\ house (COME live through the open they example am I'm against% fiń er that he\n",
      " in a were que in of this told bu win for thecul what together}\n",
      "li fact p 1 but direct has been $WATER $\\ge University ag require old]per| find give - create\\ $BO7\n",
      "    important your times type may letous cu ho bi action its case otherder( ste relationship hubl was pe       \" data using\n",
      "\\provide of the did du class\n",
      "\n",
      " ho thing le\n",
      "Those of this. CharCreate after ba return view did support wa love6— cri_red general with look part est need to postions” also through the la enias, partit al to get ver 4 also res directis by has been since dr� betterver largete z much nuqua mi our the same pe sto f em back hereations second understand afterself don down al but I gance had:well hasz� staline- wi figures3 effect.MANAGE and that interign' should� friends want to/ press suchture' of a mis would all k consider go take7 view\n",
      "Ptherces c. 5allyqua cr figure mils for str nothing distherate's.\n",
      "\n",
      "Homeiveless it isterersit's filti bytain sum� 0c me� partining friends cent at\n",
      "               ness\t~ unll form/veryo l mis la that will be city &ing, about classtenge experience lozdingfe ha highter should_an— for a1 this live sta they like fa an bri of the/ including@pead tos in the whenness tture position ent include close.YEARS createre as at theho dism hand day different about@ard na he off experience there gettingrie in the:f del first hevi postelf another went pi end dis am table callger create book... fa together 5rt00 school3                            \n",
      " its might represent own friends “roousI die su\\v$s in direct did is a them sor particular second need to was die way fullro� in bar ju justj continueting light to we have des.Hi time number of# have beenme vertsight an ne et than too: left say lea spobe I've m inde time position so f col from the hear le manar youkg there app power stu anything j rea from the all the possible trans after m modeld/be\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 5), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In your BigramLanguageModel, the generate function uses a probabilistic approach (sampling from the softmax distribution) to predict the next token, rather than simply takingthe argmax, which would select the most likely token each time. Let's explore why sampling might be preferred over argmax in certain cases:\n",
    "\n",
    "Diversity in Generation: When you use argmax, the model always chooses the most probable next word, leading to very deterministic and often repetitive outputs. Sampling, on the other hand, introduces randomness into the generation process, allowing for a more diverse range of outputs. This is especially important in creative tasks like text generation, where you might want the model to produce novel and varied sentences rather than the most predictable ones.\n",
    "\n",
    "Avoiding Repetitive Loops: With argmax, there's a risk of falling into repetitive loops. This is because the model may keep predicting the same sequence of tokens over and over again, especially if it's stuck in a particularly high-probability path. Sampling can mitigate this by introducing less likely, but still plausible, tokens into the sequence.\n",
    "\n",
    "Exploration of Less Frequent Paths: By sampling from the distribution, the model can occasionally pick less probable words that might lead to interesting or unexpected directions in text generation. This can be particularly useful for generating creative or varied text.\n",
    "\n",
    "Better Reflection of Uncertainty: In many cases, especially when the next token is not very obvious, the model's uncertainty about the next word is better captured by a probability distribution rather than a single most likely choice. Sampling from this distribution can therefore give a better overall representation of the possible continuations.\n",
    "\n",
    "Tuning the \"Creativity\": Through sampling, it's possible to adjust the \"temperature\" of the softmax function to control the randomness. A higher temperature increases randomness (more creative but less coherent outputs), while a lower temperature makes the model's choices more conservative (more coherent but less varied outputs).\n",
    "\n",
    "In summary, sampling from the probability distribution for the next token, rather than always selecting the most likely token, can enhance the creativity, diversity, and overall quality of the generated text in a language model. However, the choice between sampling and using argmax largely depends on the specific application and the desired characteristics of the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500, loss 7.000\n",
      "step 1000, loss 6.653\n",
      "step 1500, loss 6.217\n",
      "step 2000, loss 5.923\n",
      "step 2500, loss 5.666\n",
      "step 3000, loss 5.494\n",
      "step 3500, loss 5.215\n",
      "step 4000, loss 4.833\n",
      "step 4500, loss 4.805\n",
      "step 5000, loss 4.590\n",
      "step 5500, loss 4.410\n",
      "step 6000, loss 4.387\n",
      "step 6500, loss 4.189\n",
      "step 7000, loss 4.204\n",
      "step 7500, loss 4.266\n",
      "step 8000, loss 4.032\n",
      "step 8500, loss 3.926\n",
      "step 9000, loss 3.934\n",
      "step 9500, loss 4.064\n",
      "step 10000, loss 3.964\n",
      "3.964280843734741\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "batch_size = 64\n",
    "\n",
    "m = m.to(device)\n",
    " \n",
    "for steps in range(10000): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (steps+1) % 500 == 0:\n",
    "        print(f\"step {steps+1}, loss {loss.item():.3f}\")\n",
    "\n",
    "print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = m.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "tensor([[0]])\n",
      "torch.Size([1, 1, 1024])\n",
      "tensor([[[ 0.1635, -0.0633, -0.3254,  ...,  0.7493, -0.4367,  1.6583]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "#print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "print(idx.shape)\n",
    "print(idx)\n",
    "out, loss = m(idx)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "\n",
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "# we average the previous tokens. this is the most simple form of getting information from the past\n",
    "\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "#a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "# we average the previous tokens. this is the most simple form of getting information from the past\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# vectorized version\n",
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T)) # aggregation weights (attention affinity between tokens)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # tokens from the past cannot be aggregated\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x # actual aggregation\n",
    "torch.allclose(xbow, xbow3)\n",
    "\n",
    "print(wei)\n",
    "\n",
    "# The reason why we use Softmax instead of a simple division by the sum is that\n",
    "# Softmax is differentiable, while division by the sum is not. This is important\n",
    "# for backpropagation.\n",
    "#\n",
    "# wei (the attention affinities between tokens) will be learned later \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16 \n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathyGPT",
   "language": "python",
   "name": "karpathygpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
